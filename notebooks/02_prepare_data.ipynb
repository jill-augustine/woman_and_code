{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import io\n",
    "import re\n",
    "import subprocess as sp\n",
    "\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "from loguru import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import waac\n",
    "import waac.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = config.ROOT_DIR\n",
    "DATA_DIR = config.DATA_DIR\n",
    "DOWNLOAD_DIR = DATA_DIR / \"download\"\n",
    "\n",
    "IMDB_REMOTE_URI_PREFIX = \"https://datasets.imdbws.com/\"\n",
    "IMDB_LOCAL_URI_PREFIX = \"/Users/jillianaugustine/Documents/GitHub/women_and_code/data/imdb/20240503_\"\n",
    "\n",
    "rng = np.random.default_rng(seed=16042024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True\n",
    "\n",
    "IMDB_BASE_URI_PREFIX = IMDB_LOCAL_URI_PREFIX if LOCAL else IMDB_REMOTE_URI_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = list(DATA_DIR.glob(\"download/*.txt\"))\n",
    "for f in text_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = DOWNLOAD_DIR / \"movie_titles.txt\"\n",
    "movie_titles_df = waac.txt_to_df(\n",
    "    fp,\n",
    "    config.raw_data_column_names[fp.name],\n",
    "    encoding=\"latin-1\"\n",
    ")\n",
    "\n",
    "\n",
    "movie_titles_df[\"movie_ID\"] = (\n",
    "    movie_titles_df[\"movie_ID\"].astype(int)\n",
    ")\n",
    "# Set to float because of missing values\n",
    "movie_titles_df[\"year_of_release\"] = (\n",
    "    movie_titles_df[\"year_of_release\"].replace(\"NULL\", None).astype(float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles_df.info()\n",
    "display(movie_titles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles_df.describe(include=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [IDMb Non-Commercial Datasets](https://developer.imdb.com/non-commercial-datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_URLS = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    \"title.episode.tsv.gz\",\n",
    "    \"title.principals.tsv.gz\",\n",
    "    \"title.ratings.tsv.gz\"\n",
    "]\n",
    "# LOCAL_IMBD_FILE_PREFIX = DATA_DIR / \"imdb\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "url = IMDB_BASE_URL_PREFIX + SUB_URLS[0]\n",
    "imdb_metadata = {}\n",
    "CHUNK_SIZE = 1_000_000\n",
    "for sub_url in SUB_URLS:\n",
    "    url = IMDB_BASE_URL + sub_url\n",
    "    total_rows = 0    \n",
    "    with pd.read_table(url, compression=\"gzip\", iterator=True) as reader:\n",
    "        i = 1\n",
    "        while True:\n",
    "            try:\n",
    "                current_rows = reader.get_chunk(CHUNK_SIZE)\n",
    "                total_rows += len(current_rows)\n",
    "                print(f\"Iteration #{i} - Total Rows: {total_rows}\")\n",
    "                i += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "    metadata_temp = {\n",
    "        \"shape\": (total_rows, current_rows.shape[1]),\n",
    "        \"dtypes\": current_rows.dtypes.astype(str).to_dict(),\n",
    "    }\n",
    "    print(sub_url)\n",
    "    print(json.dumps(metadata_temp, indent=2, ensure_ascii=False))\n",
    "    imdb_metadata[sub_url] = metadata_temp\n",
    "    \n",
    "\n",
    "with (DATA_DIR / \"imdb\" / \"imdb_metadata.json\").open(\"w\") as f:\n",
    "    json.dump(imdb_metadata, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waac.config.imdb_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data: Approach\n",
    "\n",
    "1. Find movies in movie_titles.txt that are also in the IMDB datasets\n",
    "    1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SUB_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_metadata = {}\n",
    "CHUNK_SIZE = 1_000\n",
    "imdb_data = {}\n",
    "# for sub_url in SUB_URLS\n",
    "for sub_url in SUB_URLS[1:3]:\n",
    "    url = IMDB_BASE_URI_PREFIX + sub_url\n",
    "    print(url)\n",
    "    df_temp = pd.read_table(url, compression=\"gzip\", na_values=r\"\\N\")\n",
    "    imdb_data[sub_url] = df_temp\n",
    "    # total_rows = 0    \n",
    "    # with pd.read_table(url, compression=\"gzip\", iterator=True, na_values=r\"\\N\") as reader:\n",
    "    #     i = 1\n",
    "    #     while True:\n",
    "    #         try:\n",
    "    #             current_rows = reader.get_chunk(CHUNK_SIZE)\n",
    "    #             total_rows += len(current_rows)\n",
    "    #             print(f\"Iteration #{i} - Total Rows: {total_rows}\")\n",
    "    #             i += 1\n",
    "    #         except StopIteration:\n",
    "    #             break\n",
    "    #         break  # anyway\n",
    "    # imdb_data[sub_url] = current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data[\"title.akas.tsv.gz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data[\"title.basics.tsv.gz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(i,i for i in range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = movie_titles_df.sort_values(\"year_of_release\", ascending=False).groupby(\"year_of_release\", sort=False)\n",
    "\n",
    "{k: type(v) for k, v in x}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# row = movie_titles_df.iloc[0]\n",
    "# print(row)\n",
    "n = len(movie_titles_df)\n",
    "title_basic = imdb_data[\"title.basics.tsv.gz\"]\n",
    "# Indexing for quick filtering\n",
    "title_basic_reindexed = title_basic.set_index(\"startYear\")\n",
    "title_aka = imdb_data[\"title.akas.tsv.gz\"]\n",
    "print(f\"title.basics shape: {title_basic.shape}\")\n",
    "\n",
    "matches_so_far = 0\n",
    "for i, row in enumerate(movie_titles_df.itertuples()):\n",
    "    tconst, match_source = None, None\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"# matches so far: {matches_so_far}\")\n",
    "        print(f\"{i+1}/{n}: {row}\")\n",
    "    title_basic_filtered_year = title_basic_reindexed.loc[row.year_of_release]\n",
    "    # print(f\"title.basics (filtered on year) shape: {title_basic_filtered_year.shape}\")\n",
    "    title_basic_filtered_title = title_basic_filtered_year.loc[\n",
    "        (title_basic_filtered_year.primaryTitle.str.lower() == row.title.lower()) |\n",
    "        (title_basic_filtered_year.originalTitle.str.lower() == row.title.lower())\n",
    "    ]\n",
    "    # print(f\"title.basics (filtered on year and title) shape: {title_basic_filtered_title.shape}\")\n",
    "    if len(title_basic_filtered_title) == 1:\n",
    "        # print(\"Found match based on `basic` data\")\n",
    "        tconst = title_basic_filtered_title.iloc[0].tconst\n",
    "        match_source = 1  # \"basic\"\n",
    "    elif len(title_basic_filtered_title) > 1:\n",
    "        pass\n",
    "        # # check the aka df\n",
    "        # title_aka = imdb_data[\"title.akas.tsv.gz\"]\n",
    "        # title_basic_filtered = title_aka.loc[title_aka.titleId.isin(title_basic_filtered.tconst)]\n",
    "        # print(f\"title.basics (filtered) shape: {title_basic_filtered.shape}\")\n",
    "    else:\n",
    "        # Filter the aka df for movies from that year by using the titleID\n",
    "        aka_filtered_year = title_aka.loc[title_aka.titleId.isin(title_basic_filtered_year.tconst)]\n",
    "        aka_filtered_title = aka_filtered_year.loc[\n",
    "            (aka_filtered_year.title.str.lower() == row.title.lower())\n",
    "        ]\n",
    "        if len(aka_filtered_title) == 1:\n",
    "            tconst = aka_filtered_title.iloc[0].titleId\n",
    "            match_source = 2  # \"aka\"\n",
    "            print(3)\n",
    "        elif aka_filtered_title.empty:\n",
    "            pass\n",
    "        else:\n",
    "            # We are only interested in the titleID and many languages might have the same title\n",
    "            aka_filtered_lower = aka_filtered_title.loc[:, [\"titleId\",\"title\"]]\n",
    "            for col in aka_filtered_lower:\n",
    "                aka_filtered_lower[col] = aka_filtered_lower[col].astype(str).str.lower()\n",
    "            aka_filtered_lower = aka_filtered_lower.drop_duplicates(subset=[\"titleId\", \"title\"])\n",
    "            if len(aka_filtered_lower) == 1:\n",
    "                tconst = aka_filtered_lower.iloc[0].titleId\n",
    "                match_source = 3  # \"aka after duplicates\"\n",
    "            else:\n",
    "    results.append({\"movie_ID\": row.movie_ID, \"tconst\": tconst, \"match_source\": match_source})\n",
    "    matches_so_far += (tconst is not None)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(title_basic_filtered_title.itertuples())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.title.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_files)):\n",
    "    print(text_files[i])\n",
    "    with text_files[i].open() as f:\n",
    "        for _ in range(5):\n",
    "            print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.raw_data_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_to_extract = DOWNLOAD_DIR / \"training_set.tar\"\n",
    "\n",
    "# It won't exist anymore if it has already been extracted\n",
    "if fp_to_extract.exists():\n",
    "    with tarfile.open(fp_to_extract, \"r\") as t:\n",
    "        tar_file_names = t.getnames()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tar_file_names))\n",
    "print(tar_file_names[:2])\n",
    "print(tar_file_names[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MOVIES = 100\n",
    "\n",
    "movies_to_extract = rng.choice([x for x in tar_file_names if x.endswith(\".txt\")], size=N_MOVIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(movies_to_extract))\n",
    "movies_to_extract[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tarfile.open(DATA_DIR / \"download\" / \"training_set.tar\", \"r\") as t:\n",
    "#     t.extractall(path=DATA_DIR / \"download\", members=movies_to_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/jillianaugustine/Documents/GitHub/women_and_code/data/download/movie_titles.txt\", \"r\",\n",
    "encoding=\"latin-1\") as fp:\n",
    "    print(fp)\n",
    "    lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_txt(fp: Union[str, Path], schema: Union[Dict, List]):\n",
    "    \"\"\"Load data from a text file into a dataframe.\"\"\"\n",
    "    if isinstance(fp, str):\n",
    "        fp = Path(fp)\n",
    "    if not isinstance(fp, Path):\n",
    "        raise TypeError(f\"`fp` must be a str or Path. Got {type(fp)}.\")\n",
    "\n",
    "    def _get_chunk(stream: io.IOBase):\n",
    "        # set up the first chunk\n",
    "        i = 1\n",
    "        current_line = stream.readline()\n",
    "        if match := re.match(\"^\\d+(?=:\\n)\", current_line):\n",
    "            current_line = int(match[0])\n",
    "        chunk = [current_line]\n",
    "\n",
    "        # iterate through the file, yielding chunks as necessary\n",
    "        while True:\n",
    "            current_line = stream.readline()\n",
    "            if not current_line:\n",
    "                # End of stream\n",
    "                break\n",
    "            # check if the line matches the correct pattern\n",
    "            if match := re.match(\"^\\d+(?=:\\n)\", current_line):\n",
    "                current_line = int(match[0])\n",
    "                # yielf the previous chunk and start a new one\n",
    "                logger.debug(f\"Yielding chunk {i}\")\n",
    "                yield chunk\n",
    "                i += 1\n",
    "                chunk = [current_line]\n",
    "\n",
    "        # yield the final chunk\n",
    "        logger.debug(f\"Yielding chunk {i}\")\n",
    "        yield chunk\n",
    "\n",
    "    if not isinstance(schema, (dict, list)):\n",
    "        raise TypeError(f\"`schema` must be Dict or List. Got {type(schema)}\")\n",
    "\n",
    "    # Initialise\n",
    "    df_list = []\n",
    "    if isinstance(schema, dict):\n",
    "        assert len(schema.keys()) == 1, f\"`scehma` must contain only one key. Got {len(schema.keys())}).\"\n",
    "        header_row_name = list(schema.keys())[0]\n",
    "        col_names = list(schema.values())\n",
    "    else:\n",
    "        col_names = schema\n",
    "\n",
    "    # Read data\n",
    "    if isinstance(schema, dict):\n",
    "        # df = pd.DataFrame(columns=col_names)\n",
    "        with fp.open(\"r\", encoding=\"latin-1\") as f:\n",
    "            for chunk in _get_chunk(f):\n",
    "                chunk_header = chunk.pop(0)\n",
    "                records = [line.split(\",\") for line in chunk]\n",
    "                df_temp = pd.DataFrame.from_records(records, columns = col_names)\n",
    "                df_temp.insert(0, header_row_name, chunk_header)\n",
    "                df_list.append(df_temp)\n",
    "    else:\n",
    "        with fp.open(\"r\", encoding=\"latin-1\") as f:\n",
    "            for chunk in _get_chunk(f):\n",
    "                # no chunk header expected if schema is a list\n",
    "                records = [line.split(\",\") for line in chunk]\n",
    "                df_temp = pd.DataFrame.from_records(records, columns = col_names)\n",
    "                df_list.append(df_temp)\n",
    "\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list((DATA_DIR / \"download\").glob(\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in (DATA_DIR / \"download\").glob(\"*.txt\"):\n",
    "    if fp.name != \"qualifying.txt\":\n",
    "        continue\n",
    "    schema = config.raw_data_column_names[fp.name]\n",
    "    df = load_df_from_txt(fp, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in (DATA_DIR / \"download\").glob(\"*.txt\"):\n",
    "    if fp.name != \"qualifying.txt\":\n",
    "        continue\n",
    "    with fp.open(encoding=\"latin-1\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"^\\d+(?=:\\n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [(i, pattern.match(line)) for i, line in enumerate(lines)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (line_no, movie_ID)\n",
    "starts = [(i, int(m[0])) for i, m in starts if m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [slice(starts[i][0], starts[i+1][0]) for i in range(len(starts)-1)] + [slice(starts[-1][0], None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [None for s in slices]\n",
    "n = len(slices)\n",
    "for i, s in enumerate(slices):\n",
    "    print(f\"{i+1}/{n}\") if i % 1000 == 0 else None\n",
    "    subset = lines[s]\n",
    "    # header is always movie_ID\n",
    "    movie_ID = subset.pop(0).split(\":\")[0]\n",
    "    movie_ID = int(movie_ID)\n",
    "    df_temp = pd.DataFrame.from_records(\n",
    "        [line.strip().split(\",\") for line in subset],\n",
    "        columns = config.raw_data_column_names[\"qualifying.txt\"][\"movie_ID\"]\n",
    "    )\n",
    "    df_temp = df_temp.assign(movie_ID = movie_ID)\n",
    "    df_list[i] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

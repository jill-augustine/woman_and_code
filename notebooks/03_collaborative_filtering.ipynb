{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8b9d8-b469-40ec-9ae1-85f490f3eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0746ed-b0c2-4c74-8dd7-3819db6509ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import io\n",
    "import re\n",
    "import subprocess as sp\n",
    "\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539a206-0c43-4294-abad-4c01b6acb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import waac\n",
    "import waac.config as config\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f3794-fa6f-4868-9f2f-20ce7fca9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 6545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb985b35-b4de-418e-b3b4-d70121592fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_rng = np.random.default_rng(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a6fd1-8345-4acc-abe8-89b2b5e5458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = config.DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a49855-4739-4493-b575-f8fe4f40b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = DATA_DIR / \"intermediate\" / \"netflix_to_imdb.csv\"\n",
    "\n",
    "id_mapping = pd.read_csv(fp, sep=\";\").dropna(subset=\"tconst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392bd37-ef6b-459d-9ab0-218834de921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76926312",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = DATA_DIR / \"intermediate\" / \"rating_counts.csv\"\n",
    "rating_counts_import = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31424d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts_import.info()\n",
    "rating_counts_import[\"n_reviews\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(rating_counts_import[\"n_reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbcfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(rating_counts_import[\"n_reviews\"], cumulative=True, histnorm=\"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf3001",
   "metadata": {},
   "source": [
    "merging causes around 7000 movies to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts_mapped = rating_counts_import.sort_values(\"n_reviews\", ascending = False).merge(id_mapping, how=\"inner\", on=\"movie_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ae3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up netflix df\n",
    "fp = DOWNLOAD_DIR / \"movie_titles.txt\"\n",
    "movie_titles_df = waac.txt_to_df(\n",
    "    fp,\n",
    "    config.raw_data_column_names[fp.name],\n",
    "    encoding=\"latin-1\",\n",
    ")\n",
    "movie_titles_df = movie_titles_df.astype(object)\n",
    "movie_titles_df[\"movie_ID\"] = movie_titles_df[\"movie_ID\"].astype(int)\n",
    "# Set to float because of missing values\n",
    "movie_titles_df[\"year_of_release\"] = movie_titles_df[\"year_of_release\"].replace(\n",
    "    \"NULL\", None\n",
    ")\n",
    "movie_titles_df.dropna(subset=[\"year_of_release\"], inplace=True)\n",
    "movie_titles_df.sort_values(\"year_of_release\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02307d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rating_counts_mapped.merge(\n",
    "    movie_titles_df,\n",
    "    on=[\"movie_ID\",\"title\"],\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.year_of_release.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69422e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = 0.1\n",
    "rating_counts_subset = rating_counts_mapped.iloc[:int(rating_counts_mapped.shape[0]*prop)]\n",
    "print(f\"rating_counts_mapped: {rating_counts_mapped.shape}\")\n",
    "print(f\"rating_counts_subset: {rating_counts_subset.shape}\")\n",
    "display(rating_counts_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating_counts_subset.title.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.inf == float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DIR = DATA_DIR / \"download\"\n",
    "tar_fp = DOWNLOAD_DIR / \"training_set.tar\"\n",
    "tar_fp.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "n = rating_counts_subset.shape[0]\n",
    "results = [None for _ in range(n)]\n",
    "\n",
    "with tarfile.open(tar_fp, \"r\") as t:\n",
    "    for i, name in enumerate(rating_counts_subset.filename):\n",
    "        print(f\"{i+1}/{n}: {name=}\") if i % 100 == 0 else None\n",
    "        data_stream = t.extractfile(member=name)\n",
    "        file_header = next(data_stream).decode(\"utf-8\")\n",
    "        match_file_header = re.search(r\"[0-9]+(?=:\\n)\", file_header)\n",
    "        if not match_file_header:\n",
    "            logger.warning(f\"Skipping file based on file header match: {file_header}\")\n",
    "            continue\n",
    "        df_temp = pd.read_csv(\n",
    "            data_stream, encoding=\"utf-8\", header=None,\n",
    "            names=config.raw_data_column_names[\"training_set\"][\"movie_ID\"]\n",
    "            )\n",
    "        df_temp.insert(0, \"movie_ID\", match_file_header[0])\n",
    "        results[i] = df_temp\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87138b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(x is None for x in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a707f55",
   "metadata": {},
   "source": [
    "Check if any person reviewed a movie more than once. For example on 2 dates.\n",
    "\n",
    "It is not the case but if it were, we might take the most recent review, or the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated(subset=[\"movie_ID\",\"customer_ID\"]).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted = df.pivot(columns=\"movie_ID\", index=\"customer_ID\", values=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f59f3",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc453e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aee655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a6b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384bf505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfc724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45f88791",
   "metadata": {},
   "source": [
    "---\n",
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a893bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df, min_rating: int = 1, max_rating: int = 5):\n",
    "    return (df - min_rating) / (max_rating - min_rating)\n",
    "\n",
    "def unscale(df, min_rating: int = 1, max_rating: int = 5):\n",
    "    return (df * (max_rating - min_rating)) + min_rating\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_movies = df_pivoted.shape\n",
    "# Scaling ratings to between 0 and 1, this helps our model by constraining predictions\n",
    "\n",
    "rating_matrix = scale(df=df_pivoted)\n",
    "\n",
    "sparcity = rating_matrix.notna().sum().sum() / (n_users * n_movies)\n",
    "print(f'Sparcity: {sparcity:0.2%}')\n",
    "\n",
    "# Replacing missing ratings with -1 so we can filter them out later\n",
    "rating_matrix[rating_matrix.isna()] = -1\n",
    "rating_matrix = torch.from_numpy(rating_matrix.values).to(device)\n",
    "non_zero_mask = (rating_matrix != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted.notna().mean(axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.arange(15).reshape(3,5)\n",
    "r = torch.Tensor(r)\n",
    "display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c761df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.norm(r, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d36fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation of the Frobenius norm\n",
    "display(r)\n",
    "x = r**2\n",
    "display(x)\n",
    "x = x.sum(dim=1)\n",
    "display(x)\n",
    "x = x**0.5\n",
    "display(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b27bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    \"\"\"The model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "             u_features: torch.Tensor,\n",
    "             v_features: torch.Tensor,\n",
    "             ):\n",
    "        super().__init()\n",
    "        self.u_features = u_features\n",
    "        self.v_features = v_features\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            ):        \n",
    "        return torch.sigmoid(\n",
    "            torch.matmul(self.u_features, self.v_features.t())\n",
    "        )\n",
    "    \n",
    "class Loss(nn.Module):\n",
    "    \"\"\"Calculate loss\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            # matrix: torch.Tensor,\n",
    "            # non_zero_mask: torch.Tensor = None,\n",
    "            lam_u: float = 0.3,\n",
    "            lam_v: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lam_u = lam_u\n",
    "        self.lam_v = lam_v\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            matrix: torch.Tensor,\n",
    "            non_zero_mask: torch.Tensor,\n",
    "            predicted: torch.Tensor,\n",
    "             u_features: torch.Tensor,\n",
    "             v_features: torch.Tensor,\n",
    "            ):\n",
    "        diff = (matrix - predicted)**2\n",
    "        prediction_error = torch.sum(diff*non_zero_mask)\n",
    "\n",
    "        u_regularization = self.lam_u * torch.sum(u_features.norm(dim=1))\n",
    "        v_regularization = self.lam_v * torch.sum(v_features.norm(dim=1))\n",
    "        \n",
    "        return prediction_error + u_regularization + v_regularization\n",
    "    \n",
    "\n",
    "\n",
    "class ModelTrainer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            model_class_type, # a class type\n",
    "            loss_class, # a class instance\n",
    "            optimizer_class_type, # a subclass of torch.optim.Optimizer, not a class instance\n",
    "    ):\n",
    "        super().__init()\n",
    "        self.n_features = n_features\n",
    "        self.loss_class = loss_class\n",
    "        self._model_class_type = model_class_type\n",
    "        self._optimizer_class_type = optimizer_class_type\n",
    "\n",
    "    def train(self, matrix: torch.Tensor, n_epochs: int,\n",
    "              u_features: torch.Tensor = None, v_features: torch.Tensor = None,\n",
    "               non_zero_mask: torch.Tensor = None, lr: float=0.01\n",
    "              ):\n",
    "        \n",
    "        # Scale the data if necessary. Save the min and max values for unscaling\n",
    "        min_value = matrix.min()\n",
    "        max_value = matrix.max()\n",
    "        if (min_value != 0) or (max_value != 1):\n",
    "            # Data is not 0-1 scaled\n",
    "            matrix = scale(matrix, min_rating=min_value, max_rating=max_value)\n",
    "            self.min_matrix_value = min_value\n",
    "            self.max_matrix_value = max_value\n",
    "        else:\n",
    "            self.min_matrix_value = None\n",
    "            self.max_matrix_value = None\n",
    "        \n",
    "        # Set matrix and mask\n",
    "        if non_zero_mask:\n",
    "            assert matrix.shape == non_zero_mask.shape\n",
    "        else:\n",
    "            non_zero_mask = (matrix != -1)\n",
    "        self.matrix = matrix\n",
    "        self.non_zero_mask = non_zero_mask\n",
    "\n",
    "        # Set feature vectors\n",
    "        n_users, n_movies = matrix.shape\n",
    "        if not u_features:\n",
    "            u_features = torch.randn(\n",
    "                n_users, self.n_features, requires_grad=True, device=device\n",
    "                )\n",
    "        else:\n",
    "            assert u_features.shape == (n_users, self.n_features)\n",
    "\n",
    "        if not v_features:\n",
    "            v_features = torch.randn(\n",
    "                n_movies, self.n_features, requires_grad=True, device=device\n",
    "                )\n",
    "        else:\n",
    "            assert v_features.shape == (n_users, self.n_features)\n",
    "\n",
    "        self.u_features = u_features\n",
    "        self.v_features = v_features\n",
    "\n",
    "        # Set model and optimiser\n",
    "        self.model = self._model_class_type(self.u_features, self.v_features)\n",
    "        self.optimizer = self._optimizer_class_type(\n",
    "            [self.u_features, self.v_features], lr=lr)\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            self._train()\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch: {i}/{n_epochs}\")\n",
    "                self._validate()\n",
    "        return None\n",
    "\n",
    "    def _train(self):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            predicted = self.model(self.matrix)\n",
    "            loss = self.loss_class(\n",
    "                matrix= self.matrix,\n",
    "                non_zero_mask= self.non_zero_mask,\n",
    "                predicted= predicted,\n",
    "                u_features = self.u_features,\n",
    "                v_features = self.v_features,\n",
    "            )\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def _validate(self):\n",
    "        # There is no validation dataset here so we just made predictions of the whole dataset\n",
    "        score = self.predict()\n",
    "        print(f\"Current score: {score}\")\n",
    "    \n",
    "    def predict(self, user_idx: List[int] = None, matrix: torch.Tensor = None):\n",
    "\n",
    "        # Scale if necessary\n",
    "        scaling_info_available = (\n",
    "            (self.min_matrix_value is not None) and\n",
    "            (self.max_matrix_value is not None)\n",
    "        )\n",
    "        if matrix:\n",
    "            if not ((matrix.min() >= 0) and (matrix.max() <= 1)):\n",
    "                # data is not scaled. We need to scale the data.\n",
    "                if not scaling_info_available:\n",
    "                    raise ValueError(\n",
    "                        \"Data need to be scaled but scaling properties haven't be set.\"\n",
    "                    )\n",
    "                matrix = scale(matrix,\n",
    "                               min_rating=self.min_matrix_value,\n",
    "                               max_rating=self.max_matrix_value)\n",
    "        else:\n",
    "            # self.matrix was already scaled in self.train()\n",
    "            matrix = self.matrix\n",
    "\n",
    "        # Set user_idx\n",
    "        if user_idx is None:\n",
    "            user_idx = torch.range(matrix.shape[0])\n",
    "        if isinstance(user_idx, int):\n",
    "            user_idx = [user_idx]\n",
    "        \n",
    "        # Get predictions\n",
    "        predicted_ratings, actual_ratings =  self._predict(matrix, user_idx)\n",
    "\n",
    "        # Unscale if necessary\n",
    "        if scaling_info_available:\n",
    "            predicted_ratings = unscale(\n",
    "                predicted_ratings,\n",
    "                min_rating=self.min_matrix_value, max_rating=self.max_matrix_value\n",
    "                )\n",
    "            actual_ratings = unscale(\n",
    "                actual_ratings,\n",
    "            min_rating=self.min_matrix_value, max_rating=self.max_matrix_value\n",
    "\n",
    "                )\n",
    "        return predicted_ratings, actual_ratings\n",
    "    \n",
    "    def _predict(self, matrix: torch.Tensor, user_idx: List[int]):\n",
    "\n",
    "        user_ratings = matrix[user_idx, :]\n",
    "        non_zero_mask = user_ratings != -1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = torch.sigmoid(\n",
    "                torch.mm(\n",
    "                    self.model.u_features[user_idx, :].view(-1, self.n_features),\n",
    "                    self.model.v_features.t())\n",
    "                )\n",
    "            \n",
    "        predicted_ratings = predictions.squeeze()[non_zero_mask]\n",
    "        actual_ratings = user_ratings[non_zero_mask]\n",
    "        # NOTE: These values are not scaled\n",
    "        return predicted_ratings, actual_ratings\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5619dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[slice(2),2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f94c0-0295-4af8-ba08-0fbc924a8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "downsample_id_mapping = id_mapping.sample(n, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db28c61-afd5-4db1-9855-e7decd9ec032",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = downsample_id_mapping.movie_ID.iloc[0]\n",
    "str(s).zfill(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beaf69d-74c7-4e99-b164-1127fd7f4c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for movie_id in downsample_id_mapping.movie_ID:\n",
    "    training_fp = DATA_DIR / \"download\" / \"training_set\" / f\"mv_{str(movie_id).zfill(7)}.txt\"\n",
    "    print(training_fp)\n",
    "    print(training_fp.exists())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778ccab-5d16-443e-ab8a-97fa3965fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tar_fp = DATA_DIR / \"download\" / \"training_set.tar\"\n",
    "\n",
    "with tarfile.open(training_tar_fp) as tf:\n",
    "    names = tf.getnames()\n",
    "    # \"training_set/mv_1234567.txt\" ...\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9980ac-314a-456d-8789-9f5130b4a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tar_fp = DATA_DIR / \"download\" / \"training_set.tar\"\n",
    "\n",
    "with tarfile.open(training_tar_fp) as tf:\n",
    "    for movie_id in downsample_id_mapping.movie_ID:\n",
    "        member = f\"training_set/mv_{str(movie_id).zfill(7)}.txt\"\n",
    "        print(f\"{member=}\")\n",
    "        x = tf.extractfile(member)\n",
    "        print(x)\n",
    "        print(type(x))\n",
    "        # read header row\n",
    "        y0 = next(x)\n",
    "        # read the rest\n",
    "        d = pd.read_csv(x, encoding=\"utf-8\", header=None, names=config.raw_data_column_names[\"training_set\"][\"movie_ID\"])\n",
    "        # y = [_ for _ in x]\n",
    "        # contents = x.read().decode(\"utf-8\")\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541ab7e-c991-4757-8268-1399b9462e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d\n",
    "\n",
    "# todo:\n",
    "# add a column called movieID, spread the df so customer is index, movieID is columns, values = rating. (date is removed)\n",
    "# maybe have to drop duplicates on [customer_ID,movieID] first incase someone rated the same movie twice\n",
    "\n",
    "# This is the input for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2998f-b242-4134-9f2e-98bb79331719",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([4,5,5,np.NaN, 5,5, np.NaN, np.NaN, np.NaN]].reshape(3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3376133-0e89-4c94-bd8b-d682c6a11d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b326a9-4ba3-4007-9e7b-9e17a5fa1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8fbe8-9f13-4c7f-ae72-f7ee708d549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f066c2c-df3e-4603-be4e-f91eceefa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf7c7f-0220-423b-8b74-2363b6cf2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "_bytes.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85117c-8118-4bce-84b3-65778fd9c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list((DATA_DIR / \"download\" / \"training_set\").glob(\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6086e0-0cf3-449c-89ec-8b94b1e30add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
